\documentclass[traditabstract]{aa}

\usepackage{graphicx}
\usepackage{txfonts}
\usepackage{natbib}
\usepackage{xspace}
\usepackage{url}
\bibpunct{(}{)}{;}{a}{}{,}

\begin{document}

\title{PyCS time delay measurement on TDC1}
\author{ In alphabetical order:
V. Bonvin\inst{\ref{epfl}} \and
F. Courbin\inst{\ref{epfl}} \and
M. Tewes\inst{\ref{bonn}}
}

\institute{
Laboratoire d'astrophysique, Ecole Polytechnique F\'ed\'erale de Lausanne (EPFL), Observatoire de Sauverny, 1290 Versoix, Switzerland \label{epfl}
\and
Argelander-Institut f\"ur Astronomie, Auf dem H\"ugel 71, D-53121 Bonn, Germany \label{bonn}
}

\date{\today}
%\abstract{}

\maketitle


% -Authors and affiliations
% -Description of your method(s)
% -Description of internal testing and/or references to prior applications of the method
% -A description of the strategy you have used to identify catastrophic failures
%We realize there is not a lot of time between now and the deadline, but it is important that this paragraph of text be written blind as well. Please keep it short! This is only meant to be a brief summary for each team. We encourage you to write up your method separately if you wish so.

\section{PyCS method descriptions}

The PyCS team has made submissions using 3 time-delay measurement methods: \emph{d3cs}, \emph{spl}, and \emph{sdi}. The last two of them build upon initial estimations provided by the first one. We summarize the methods in the following. 

\subsection{d3cs: D3 curve shifting}
This first method is based on human inspection of the light curves, in the spirit of citizen science projects. We have developed a dedicated browser-based visualization interface, using the D3.js JavaScript library\footnote{Data Driven Documents, \url{http://www.d3js.org/}} by \citet{d3}. Our tool is now publicly available online\footnote{\url{http://www.astro.uni-bonn.de/~mtewes/d3cs/tdc1/} (See ``Read me first'' for help)} [Note: not sure if you/we want to leave this URL in the TDC paper, but feel free to explore and register estimates!]. The main motivations behind this time-costly yet simple approach was to obtain, for each curve pair, (1) a rough initial estimate for the time delay and associated uncertainty, and (2) a robust characterization of the confidence that this estimate is not a catastrophic error. Our interface allows users to interactively shift the light curves in time, magnitude, and flux, and to zoom in on interesting sections of the data. This enables users to estimate both the time delay and an associated uncertainty. Importantly, the interface also asks to pick a confidence category for the proposed solution, among 4 choices:
\begin{enumerate}
\item ``doubtless'' if a catastrophic error can be excluded with a very high confidence,
\item ``plausible'' if the solution yields a good fit and no other solutions are seen,
\item ``multimodal'' if the proposed solution is only one among two or more possible solutions,
\item ``uninformative'' if the data does not reveal any delay.
\end{enumerate}
Only a small circle of scientists has participated to the visual inspection of TDC1, yet we obtained at least two estimates by different people for each curve pair. Different users have taken significantly different typical amount of time to register their estimates, from $\approx 25$ s per pair for ``fast'' users (vb) to $> 60$ s per pair for ``slow'' users (mt). We have included submissions using raw estimates from such different users in our blind submission set. The database of d3cs estimates was then carefully ``reduced'' to a single estimate per pair, resolving any conflicts between estimates in a conservative way. A key result of this step is a sample of 1628 ``doubtless'' time-delay estimates, which, as we hope, is \emph{free} from any catastrophic outliers. Through this experiement, we have demonstrated that such an approach remains well tractable for about 5000 light curves.

\subsection{spl: free-knot spline fit}
This method can be seen as a simplified version of the ``free-knot spline technique'' described in \citet{pycs} and implemented in the PyCS software package. Using the d3cs estimate as starting point, the method simultaneously fits one single spline representing the intrinsic QSO variability and a smoother ``extrinsic'' spline representing the differential microlensing variability to the light curves. During this iterative process, the curves are shifted in time so as to optimize the fit. We repeat this fit 20 times, starting form different initial conditions, to test and improve the robustness of the resulting delay against local minima. Such a model fit is then used to generate 40 simulated noisy light curves with a range of true time delays around the best-fit solution. By rerunning the spline fit on these simulated curves, and comparing the resulting delays with the true input time delays, the delay measurement uncertainty is estimated.

The \emph{spl} method for TDC1 is simpler, faster, and significantly less conservative in the uncertainty estimation than the free-knot spline technique as applied to COSMOGRAIL data in \citet{Tewes:2013iz} and \citet{RathnaKumar:2013eu}. In particular:
\begin{enumerate}
\item The temporal density of spline knots controling the flexibility of the intrinsic spline is computed from the signal-to-noise ratios measured on the two light curves, using an empirical calibration. These signal-to-noise ratios compare the typical amplitude of variability observed in a window of 50 to 75 days with the typical variability from one epoch to the next, that is the ``scatter''. For the extrinsic spline, the knot density is fixed to be the same for all TDC1 pairs.
\item We do not inject any fast microlensing signal into the simulated light curves on top of the smooth microlensing fit determined using the data. Only plain white noise is added to the generative model. 
\item We do not analyse the delay measurement errors on the simulated curves as a function of true delay. Instead, only the RMS error of these delay measurements is used as our total uncertainty estimate. 
\item Finally, we do not manually fine-tune any parameters or correct for problematic model fits. 
\end{enumerate}
These simplifications, and the fact that we did not  remove any form of ``safety margin''

. On purpose, to aim for a $\chi^2$ of 1.0





 and applied in 

 and sdi are simpler, faster, and also less conservative in their uncertainty estimation. There is no "safety margin" left.



After the D3CS stage, there is no more human “tweaking”, it's all algorithms. So for instance we do not redo pycs simulations because the spline fit looks weird etc.
Simplifications of spline technique

no fast ML signal is added to the simulated light curves. Only plain regular white noise.
We draw only 40 simulated curves with randomized “true” delays, but do not split up the analysis by bins of true delay. We just take the RMS error of the delay measurements on these sims as our total error.
So it does not try to be overly careful or conservative. We did not leave in any “security margin” in the error-bar computation, and expect our spl error bars to be on the low side.


The entire process takes about 5 CPU-minutes for an average TDC1 pair, which is of the order of 50 times faster than the analysis of 



\subsection{sdi}


D3CS decides about the confidence we have in a time delay estimation. PyCS is not allowed to yield estimates that are not compatible with D3CS. “Do not trust a delay which you can't see by eye”.
PyCS CPU time (per curve, starting from a good initial time delay), 5 CPU-min. Much faster than the analysis of cosmograil curves.
D3CS time
After the D3CS stage, there is no more human “tweaking”, it's all algorithms. So for instance we do not redo pycs simulations because the spline fit looks weird etc.
Simplifications of spline technique

no fast ML signal is added to the simulated light curves. Only plain regular white noise.
We draw only 40 simulated curves with randomized “true” delays, but do not split up the analysis by bins of true delay. We just take the RMS error of the delay measurements on these sims as our total error.
So it does not try to be overly careful or conservative. We did not leave in any “security margin” in the error-bar computation, and expect our spl error bars to be on the low side.


\section{}

We have 3 methods: d3cs, spl, and sdi, which act in 2 stages:

Stage 1: d3cs yields rough initial time delay estimates (one per pair) as well as a characterization of the confidence that this delay estimate is not a catastrophic error. This confidence is encoded in 3 levels: "dou" for doubtless, "pla" for plausible, "mul" for multimodal.

Stage 2: spl or sdi build on top of the d3cs results, to increase accuracy and precision, but without changing the confidence level. The spl and sdi algorithms are inspired by the spline and regdiff methods from COSMOGRAIL, respectively. However, spl and sdi are simpler, faster, and also less conservative in their uncertainty estimation. There is no "safety margin" left.

For all three methods, our submissions are named according to the same convention: "pycs\_tdc1\_A-B-C-D.dt", where
 * A: method, that is d3cs, spl or sdi
 * B: method parameters ("vanilla" corresponds to the default settings, which we think are best or simplest)
 * C: confidence categories: "dou" for doubtless, "doupla" for both doubtless and plausible light curve pairs
 * D: filters that reject some delays following different criteria ("full" corresponds to no filter).

Submissions that share the same method and method parameters differ only in the selection/rejection of delays. The numerical values for delays and errors in these submissions are identical.

We analyzed all TDC1 curves, and so our submissions are "filled" with -99 flags. The only exception to this is the method "d3cs-mt", that saw only part of TDC1. Its submission files therefore do not cover the full TDC1.

If we would have to blindly select a single submission which is at the same time very ambitious in terms of P, f, and with a median chi2 hopefully close to 1, it would be
pycs\_tdc1\_spl-median-doupla-splagree.dt
It consists of the median of estimates from different spl variants, rejecting those delays for which the spl variants do not agree well.


Our submissions are grouped in various methods, each of them using different method parameters, then splitted into confidence categories, on which we finally apply different filters. That structure is reflected in the name of each submission, presented as pycs\_tdc1\_method-parameters-confidence-filter.dt

The first method, d3cs (for D3 Curve Shifting, D3 being a JavaScript applet), 
The second method, spl (for Spline), use the d3cs method estimations as input parameters. For each pair, shifted by the delay found with d3cs, the method fits a spline through both lightcurves, and adapt the delay to minimise the residuals of the data with respect to the spline. The presence of microlensing in one of the two lightcurves is taken into account when fitting the spline. The values of the delay and error are computed by applying the method on numerous copies and simulations of the original curves.

The third method, sdi (for Spline DIfference), also use the d3cs method estimations as input parmaeters. For each pair, the method fits a spline in each lightcurve, and then minimize the difference between the two splines, by shifting them in time around the d3cs input estimation. Again, the values of the delay and error are computed on copies and simulation of the original curves.




\begin{acknowledgements}
VB and FC are supported by the Swiss National Science Foundation (SNSF). MT acknowledges support by the DFG grant Hi 1495/2-1.
\end{acknowledgements}



\bibliographystyle{aa}
\bibliography{pycs_description}

\end{document}



